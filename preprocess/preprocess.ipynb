{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('train_data.xlsx')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing  functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Clear out HTML characters \n",
    "import html\n",
    "def clear_HTMLcharacters(text):\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 2- Encoding UTF-8 \n",
    "def encode_utf8(text):\n",
    "    text = text.encode('ascii','ignore')\n",
    "    encode_tweet=text.decode(encoding='UTF-8')\n",
    "    return encode_tweet\n",
    "\n",
    "\n",
    "# 3- Extract Hashtags\n",
    "def extract_hashtags(text):\n",
    "    hashtag_list = []\n",
    "    for word in text.split():\n",
    "        if word[0] == '#':\n",
    "            hashtag_list.append(word[1:])\n",
    "    return hashtag_list\n",
    "\n",
    "\n",
    "# 4- Removing URLs, Hashtags and Styles\n",
    "def remove_URL_Hashtags_Style(text):  \n",
    "    text = re.sub(r'https?:\\/\\/.\\S+', \"\", text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 5- Convert Numbers into words\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "def convert_number(text):\n",
    "    temp_str = text.split()\n",
    "    new_string = []\n",
    "    for word in temp_str:\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "    temp_str = ' '.join(new_string)\n",
    "    temp_str = re.sub(r'\\d+', '', temp_str)\n",
    "    return temp_str\n",
    "\n",
    "\n",
    "# 6- Text Lowercase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# 7- Replace Contraction \n",
    "Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "def replace_contraction(text):\n",
    "    for key,value in Apos_dict.items():\n",
    "            if key in text:\n",
    "                text=text.replace(key,value)\n",
    "    return text     \n",
    "\n",
    "\n",
    "# 8- Split attached words\n",
    "def split_attached_words(text):\n",
    "    text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",text) if s])\n",
    "    return text\n",
    "\n",
    "\n",
    "# 9- Remove More Than twice repeat letter\n",
    "import itertools\n",
    "def remove_more_than_twice(text):\n",
    "    #One letter in a word should not be present more than twice in continuation\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    return text\n",
    "\n",
    "# 10- Spell Checking\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "def spell_checking(text):\n",
    "    text=spell(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 11- Slang lookup\n",
    "file=open(\"slang.txt\",\"r\")\n",
    "slang=file.read()\n",
    "slang=slang.split('\\n')\n",
    "\n",
    "def slang_lookup(text):\n",
    "    text_tokens=text.split()\n",
    "    slang_word=[]\n",
    "    meaning=[]\n",
    "    for line in slang:\n",
    "        temp=line.split(\"=\")\n",
    "        slang_word.append(temp[0])\n",
    "        meaning.append(temp[-1])\n",
    "    for i,word in enumerate(text_tokens):\n",
    "        if word in slang_word:\n",
    "            idx=slang_word.index(word)\n",
    "            text_tokens[i]=meaning[idx]\n",
    "         \n",
    "    text=\" \".join(text_tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 12- Remove Punctuations\n",
    "import string   \n",
    "def remove_punctuations(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    " \n",
    "    \n",
    "\n",
    "# 13- Remove whitespaces\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "  \n",
    "    \n",
    "# 14- Remove  stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "en_stops = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in text_tokens if word not in en_stops]\n",
    "    text = \" \".join(filtered_text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 15- Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    text  =\" \".join(stems)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# total preprocess tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(text):\n",
    "    text = clear_HTMLcharacters(text)\n",
    "    text = encode_utf8(text)\n",
    "    text = remove_URL_Hashtags_Style(text)\n",
    "    text = split_attached_words(text)\n",
    "    text = remove_more_than_twice(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = convert_number(text)\n",
    "    text = replace_contraction(text)\n",
    "    text = remove_punctuations(text)      \n",
    "    text = slang_lookup(text)      \n",
    "    text = remove_whitespace(text)      \n",
    "    text = remove_stopwords(text)\n",
    "    text = spell_checking(text)\n",
    "    text = stem_words(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in df[\"Text\"]:\n",
    "    objects = []\n",
    "    tweet = i\n",
    "    try:\n",
    "        preprocess_tweet = preprocess_tweets(tweet)\n",
    "        df.loc[[count], \"preprocess_tweets\"] = preprocess_tweet\n",
    "        print(\"tweet number \", count, \"is preprocessed\")\n",
    "        count = count + 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        count = count + 1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Image classes with EfficientNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "model.eval()\n",
    "\n",
    "tfms = transforms.Compose([transforms.Resize(224),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
    "\n",
    "labels_map = json.load(open('labels.txt'))\n",
    "labels_map = [labels_map[str(i)] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "for i in range(75,500):\n",
    "    objects = []   \n",
    "    print(i)\n",
    "    url = df[\"Media URLs\"][i]\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img = tfms(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "             outputs = model(img)\n",
    "        for idx in torch.topk(outputs, k=10).indices.squeeze(0).tolist():\n",
    "            prob = torch.softmax(outputs, dim=1)[0, idx].item()\n",
    "            label=labels_map[idx]\n",
    "            objects.append(label)\n",
    "        print(objects)\n",
    "        df.loc[[i], \"object\"] = (' '.join(objects))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df.loc[[i], \"object\"] = (' '.join(objects))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['preprocess_tweets','object','Emoji']]\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"preprocess_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
