{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"preprocess_data.xlsx\")\n",
    "data = data[['preprocess_tweets','object','Emoji']]\n",
    "data = data.dropna()\n",
    "print('We have',len(data), 'tweets in the dataset')\n",
    "labels = data.Emoji.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to workwith 5 most frequent emoji\n",
    "#n = 5\n",
    "#top_5_emoji = data['Emoji'].value_counts()[:n].index.tolist()\n",
    "#mask = data['Emoji'].isin(top_5_emoji)\n",
    "#data = data[mask]\n",
    "#data = data.reset_index(drop=True)\n",
    "#print(top_5_emoji)\n",
    "\n",
    "#dict={1:0,7:1,0:2,2:3,5:4}\n",
    "#data = data.replace({\"Emoji\": dict})\n",
    "#labels = data.Emoji.values\n",
    "#print('We have',len(data), 'not nan tweet in the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA topic modelig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=1,                         # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             max_features=1400,                # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized1 = vectorizer.fit_transform(data['preprocess_tweets'])\n",
    "data_vectorized2 = vectorizer.fit_transform(data['object'])\n",
    "print(data_vectorized1.shape)\n",
    "print(data_vectorized2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=90,           # Number of topics\n",
    "                                      learning_method='online',\n",
    "                                      random_state=0,            # Random state\n",
    "                                      n_jobs = -1                # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(np.concatenate((data_vectorized1.toarray(),data_vectorized2.toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(text):\n",
    "    # Step 1: Vectorize transform\n",
    "    mytext_4 = vectorizer.transform(text)\n",
    "    # Step 2: LDA Transform\n",
    "    topic_probability_scores = lda_model.transform(mytext_4)\n",
    "    return topic_probability_scores\n",
    "\n",
    "\n",
    "prob_scores_q1 = predict_topic(text = data['preprocess_tweets'])\n",
    "print(prob_scores_q1.shape)\n",
    "prob_scores_q2 = predict_topic(text = data['object'])\n",
    "print(prob_scores_q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepration for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# function to tokenize and generate input ids for the tokens\n",
    "# returns a list of input ids\n",
    "def prep_data(ques1):\n",
    "    \n",
    "  all_input_ids = []\n",
    "  \n",
    "  for q1 in ques1:\n",
    "    \n",
    "    q1 = '[CLS] ' + q1 \n",
    "    tokens = tokenizer.tokenize(q1)\n",
    "    \n",
    "    # input ids are generated for the tokens (one question pair)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # input ids are stored in a separate list\n",
    "    all_input_ids.append(input_ids)\n",
    "    \n",
    "  return all_input_ids\n",
    "\n",
    "all_input_ids = prep_data(data['preprocess_tweets'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sentence to have equal size \n",
    "max_len = 0\n",
    "for i in all_input_ids: \n",
    "    if max_len < len(i):\n",
    "        max_len = len(i)\n",
    "\n",
    "# max len of sentences \n",
    "n = max_len\n",
    "print(max_len)\n",
    "pad_input_ids = pad_sequences(all_input_ids, maxlen=n, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in pad_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new accuracy\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import  BertModel\n",
    "\n",
    "    \n",
    "class my_BERT(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "    def __init__(self,emb_size, topic_num):\n",
    " \n",
    "        super().__init__()\n",
    "         # pretrain BERT\n",
    "        self.tbert  = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # output shape of classifier = 5 for 5 emoji and 10 for 10 emoji\n",
    "        self.classifier = torch.nn.Linear(emb_size + topic_num + topic_num,10) \n",
    "\n",
    "    def forward(self, b_input_ids, attention_mask, topics, token_type_ids=None):\n",
    "         \n",
    "        _, bert = self.tbert(b_input_ids,attention_mask)  # cls output of BERT\n",
    "        output  = self.classifier(torch.cat((bert,topics),-1))\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_BERT(emb_size=768,topic_num=90)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs,validation_inputs,train_labels,validation_labels=train_test_split(np.concatenate((pad_input_ids,prob_scores_q1,prob_scores_q2),axis=-1),labels,random_state=2021,test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, pad_input_ids,random_state=2021, test_size=0.2)\n",
    "\n",
    "train_topics = torch.tensor(train_inputs[:,n:])\n",
    "train_inputs = torch.tensor(train_inputs[:,0:n])\n",
    "\n",
    "validation_topics = torch.tensor(validation_inputs[:,n:])\n",
    "validation_inputs = torch.tensor(validation_inputs[:,0:n])\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "batch_size = 10\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels,train_topics)\n",
    "train_dataloader = DataLoader(train_data,batch_size=batch_size,shuffle = True)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels,validation_topics)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size,shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from torch.optim.lr_scheduler import ExponentialLR,StepLR\n",
    "\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if n in no_decay], 'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    \n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=0.005, warmup=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size = 3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_loss_set = []\n",
    "train_acc_set = []\n",
    "val_loss_set= []\n",
    "val_acc_set=[]\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "  print('-'*8+\"epoch:\"+str(epoch)+'-'*8)\n",
    "    \n",
    "  # Training \n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  train_loss = 0\n",
    "  train_accuracy = 0\n",
    "  nb_tr_steps = 0\n",
    "  nb_tr_examples = 0\n",
    "\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "       \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_topics= batch\n",
    "     \n",
    "    b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "    b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "    b_labels = b_labels.type(torch.LongTensor)\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_input_mask = b_input_mask.to(device)\n",
    "    b_labels = b_labels.to(device)\n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(b_input_ids,b_input_mask,b_topics.float().to(device))\n",
    "\n",
    "    loss = criterion(outputs, b_labels)\n",
    "    loss.backward(loss)\n",
    "   \n",
    "    # train accuracy\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    b_labels = b_labels.to('cpu').numpy()\n",
    "    tmp_train_accuracy = accuracy(outputs, b_labels)\n",
    "    train_loss += loss.item()\n",
    "    train_accuracy += tmp_train_accuracy\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "  scheduler.step()\n",
    "  train_loss_set.append((train_loss/nb_tr_examples))     \n",
    "  train_acc_set.append((train_accuracy/nb_tr_examples))\n",
    "\n",
    "  \n",
    "      \n",
    "  # Validation\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps = 0\n",
    "  nb_eval_examples = 0  \n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    val_input_ids, val_input_mask, val_labels, val_topics = batch\n",
    "    \n",
    "\n",
    "    val_input_ids  = val_input_ids.type(torch.LongTensor)\n",
    "    val_input_mask = val_input_mask.type(torch.LongTensor)\n",
    "    val_input_ids  = val_input_ids.to(device)\n",
    "    val_input_mask = val_input_mask.to(device)\n",
    "    val_labels = val_labels.to(device)\n",
    "\n",
    "    \n",
    "    # Forward pass, calculate logit predictions\n",
    "    with torch.no_grad():\n",
    "        val_output = model(val_input_ids,val_input_mask,val_topics.float().to(device))\n",
    "    \n",
    "    tmp_eval_loss = criterion(val_output, val_labels)\n",
    "    \n",
    "    val_output = val_output.detach().cpu().numpy()\n",
    "    val_labels = val_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(val_output, val_labels)\n",
    "    \n",
    "    eval_loss += tmp_eval_loss.item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "\n",
    "    nb_eval_examples += val_input_ids.size(0)\n",
    "    \n",
    "  val_loss_set.append((eval_loss / nb_eval_examples))\n",
    "  val_acc_set.append((eval_accuracy / nb_eval_examples))\n",
    "    \n",
    "  if val_acc_set[-1] >= max(val_acc_set):\n",
    "        print('saving model ...')\n",
    "        torch.save(model, \"my_checkpoint2.pth.tar\")\n",
    "\n",
    "\n",
    "  print(\"epoch train loss:\\t\"+ str(round((train_loss_set[-1]),6))    + \"\\tepoch train acc:\\t\" + str(round((train_acc_set[-1]),6)))\n",
    "  print(\"epoch validation loss:\\t\" + str(round((val_loss_set[-1]),6)) + \"\\tepoch validation acc:\\t\"  + str(round((val_acc_set[-1]),6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist():\n",
    "    plt.plot(train_acc_set)\n",
    "    plt.plot(val_acc_set)\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_hist():\n",
    "    plt.plot(train_loss_set)\n",
    "    plt.plot(val_loss_set)\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(\"preprocess_test_data.xlsx\")\n",
    "test_data = test_data[['preprocess_tweets','object','Emoji']]\n",
    "test_data = test_data.dropna()\n",
    "print('We have',len(test_data), 'tweets in the test dataset')\n",
    "labels = test_data.Emoji.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_scores_q1 = predict_topic(text = test_data['preprocess_tweets'])\n",
    "print(prob_scores_q1.shape)\n",
    "prob_scores_q2 = predict_topic(text = test_data['object'])\n",
    "print(prob_scores_q2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = prep_data(test_data['preprocess_tweets'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_input_ids = pad_sequences(all_input_ids, maxlen=n, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in pad_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topics = torch.from_numpy(np.concatenate((prob_scores_q1,prob_scores_q2),axis=-1))\n",
    "test_inputs = torch.tensor(pad_input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels,test_topics)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"my_checkpoint2.pth.tar\", map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_set  = []\n",
    "test_acc_set   = []\n",
    "\n",
    "test_loss, eval_acc = 0, 0\n",
    "nb_test_steps = 0\n",
    "nb_test_examples = 0\n",
    "step = 0 \n",
    "\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    test_input_ids, test_input_mask, test_labels, test_topics = batch\n",
    "    \n",
    "    test_input_ids  = test_input_ids.type(torch.LongTensor)\n",
    "    test_input_mask = test_input_mask.type(torch.LongTensor)\n",
    "    test_input_ids  = test_input_ids.to(device)\n",
    "    test_input_mask = test_input_mask.to(device)\n",
    "    test_labels     = test_labels.to(device)\n",
    "     \n",
    "    test_output   = model(test_input_ids,test_input_mask,test_topics.float().to(device))\n",
    "    tmp_test_loss = criterion(test_output, test_labels)\n",
    "    test_output   = test_output.detach().cpu().numpy()\n",
    "    test_labels   = test_labels.to('cpu').numpy()\n",
    "    tmp_test_acc  = accuracy(val_output, val_labels)\n",
    "    \n",
    "    test_loss += tmp_test_loss.item()\n",
    "    test_acc  += tmp_test_acc\n",
    "    \n",
    "    nb_test_examples += test_input_ids.size(0)\n",
    " \n",
    "\n",
    "test_loss_set.append((test_loss / nb_eval_examples))\n",
    "test_acc_set.append((test_acc / nb_eval_examples))\n",
    "       \n",
    "print(\"test loss:\\t\" + str(round((test_loss_set),6)) + \"\\t test acc:\\t\"  + str(round((test_acc_set,6)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
